# Group Membership

В этом задании вам предстоит реализовать распределенный сервис для отслеживания состава группы. Это один из ключевых компонентов многих распределенных систем.

Пусть имеется распределенная система, состоящая из _N_ узлов, на каждом из которых запущен процесс. У каждого процесса есть уникальный идентификатор _id_. Будем говорить, что все работающие в некоторый момент процессы образуют _группу_, то есть являются ее участниками. В ходе работы системы процессы могут запускаться или останавливаться, тем самым вступая в группу и покидая её. Таким образом, состав участников группы может изменяться со временем. Сервис, который вам надо реализовать, должен позволять процессам узнавать текущий состав группы, а также выполнять вход и выход из группы.

Помимо явного выхода из группы (например, при штатной остановке администратором), процессы могут покидать группу из-за отказов. В нашей системе возможны следующие виды отказов:
- _Падение процесса или всего узла_. При таком отказе процесс прекращает свое выполнение, теряя имевшееся у него состояние. Позднее процесс может быть перезапущен (с тем же _id_ и чистым состоянием) и заново подключен к группе. Работающий процесс будем называть _живым_.
- _Потеря сообщений сетью_ между некоторыми парами узлов, в обоих или только в одном направлении. В результате таких отказов живые процессы могут оказаться полностью или частично изолированы от других процессов. Отказы сети могут носить временный характер, после чего связь восстанавливается.

Ваша реализация должна обнаруживать описанные выше отказы и обрабатывать их. Для этого вам потребуется реализовать _детектор отказов_ (см. лекцию 6). Ваш детектор должен обладать свойством _полноты_ - если процесс стал недоступным из-за отказа, то в конечном счете все участники группы признают его отказавшим. Также постарайтесь найти хороший баланс между скоростью обнаружения отказов, точностью детектора и сопутствующими накладными расходами.

Упавшие или недоступные ни с одного из остальных участников группы процессы должны автоматически исключаться из состава группы. Таким образом, в состав группы должны входить только живые процессы, способные общаться по сети (в обе стороны) с хотя бы одним другим участником группы. Если упавший процесс позднее перезапускается, то он заново выполняет вход в группу под тем же _id_, после чего остальные процессы должны вернуть его в свой список участников. Если недоступный из-за отказа сети процесс позднее становится доступным, то другие участники должны обнаруживать это и аналогично возвращать процесс в свой список. 

Не требуется обеспечивать _строгую согласованность_ списков участников группы между процессами. То есть допустимо, если в некоторый момент времени списки на разных процессах отличаются (например, отказавший процесс может кое-где еще выдаваться как участник группы). Главное чтобы через некоторое время после отказа или его устранения, а также входа/выхода участника, состав группы стабилизировался и стал одинаковым на всех её текущих участниках. Это называется _согласованностью в конечном счёте_.

Из-за сетевых отказов система может распасться на несколько изолированных подгрупп. Такая ситуация называется _разделением сети_. Простейшим частным случаем разделения является отключение от сети одного узла. Разделение может быть асимметричным, когда связь между частями системы отсутствует только в одну сторону. Ваша реализация должна уметь работать в условиях разделения сети, позволяя каждому живому процессу отслеживать состав участников в его подгруппе. Например, если процесс оказался отключен от сети, то он будет видеть в участниках группы только себя, а остальные процессы будут видеть в группе всех кроме него. Когда сеть восстанавливается, то в конечном счёте все живые процессы должны увидеть всех в участниках группы.

Чтобы получить максимальный балл, требуется также обеспечить хорошую масштабируемость полученного решения. А именно, при увеличении числа узлов в _X_ раз накладные расходы (сетевой трафик и число передаваемых сообщений) должны увеличиваться не более чем в _2X_ раз.  В тестах не учитываются расходы на первоначальную сборку группы, только расходы на её поддержание в ситуации без отказов и с ними. Кроме того, нагрузка на процессы (в плане числа обрабатываемых сообщений) должна распределяться равномерно - отношение max/min нагрузки по всем процессам не должно превышать 5.

Если вам плохо понятны некоторые требования, изучите соответствующие тесты - это часть условия.

## Реализация

Для реализации и тестирования решения используется фреймворк AnySystem (см. материалы первого семинара). В папке `solution` размещена заготовка для решения [member.py](solution/member.py). Вам надо доработать реализацию процесса в классе `GroupMember` так, чтобы проходили все тесты.

При инициализации процессу передается его уникальный _id_. Процесс должен поддерживать обработку следующих локальных сообщений:
- _JOIN_ - команда присоединиться к группе. В результате обработки этой команды процесс должен стать участником группы. В поле `seed` передается идентификатор одного из живых участников группы. Если в `seed` находится идентификатор самого процесса, то он должен создать новую пустую группу и добавить в неё себя. Ответ на это сообщение не нужен.
- _LEAVE_ - команда покинуть группу. В результате обработки этой команды процесс должен перестать быть участником группы. Ответ на это сообщение не нужен.
- _GET_MEMBERS_ - запрос списка участников группы. В ответ на запрос процесс должен отправить локальное сообщение _MEMBERS_ со списком идентификаторов процессов, входящих сейчас в группу (см. заготовку). **Важно!** Данный запрос должен обрабатываться исключительно локально, без взаимодействия с другими процессами. При поступлении запроса надо сразу выдать ответ с хранимым локально списком. На соблюдение этой семантики есть отдельный тест.

Для взаимодействия между процессами вы можете использовать любые собственные типы сообщений.

**Важно!** Для измерения прошедшего времени в коде процесса используйте метод `ctx.time()`, возвращающий локальное время "внутри симуляции". Использовать обычные способы, например `time.time()`, некорректно, так как скорость течения времени в симуляции отличается от реального времени. Также не стоит опираться на синхронизацию часов в системе - у каждого узла свои локальные часы, и показания `ctx.time()` у разных процессов могут расходиться.

В коде процесса можно использовать генератор случайных чисел из `random`, но не следует его явно инициализировать с помощью `random.seed()`. Это уже делается в тестах на основе переданного random seed, а дополнительная инициализация может нарушить воспроизводимость результатов.

## Тестирование

### Локальное тестирование

Тесты находятся в папке `tests`. Есть два варианта их запуска.

Рекомендуемый вариант запуска тестов - через готовый Docker-образ. В этом случае используемое окружение будет аналогично тестирующей системе. Для запуска тестов выполните команду:

```commandline
python3 ../cli.py test [ЗДЕСЬ МОЖНО УКАЗАТЬ ОПЦИИ]
```

Вы также можете запустить тесты, скомпилировав их локально с помощью компилятора Rust. Такой вариант может быть удобен, если вы хотите лучше изучить или доработать тесты. Скомпилируйте тесты с помощью команды `cargo install --path tests`. Для запуска тестов выполните команду:

```commandline
distsys-membership [ЗДЕСЬ МОЖНО УКАЗАТЬ ОПЦИИ]
```

Запустить только один из тестов можно с помощью опции `-t` (например, `... -t "PROCESS CRASH"`). По умолчанию вывод тестов не содержит трассы (последовательности событий во время выполнения каждого из тестов), а только финальную сводку. Включить вывод трасс можно с помощью флага `-d`. Все доступные опции можно посмотреть с помощью флага `-h`. Часть из них уже должна быть вам знакома по прошлым заданиям. Число тестов `CHAOS MONKEY` в тестирующей системе увеличено до 1000 (опция `-m 1000`).

Если тест MODEL CHECKING выполняется очень долго, то вы можете временно отключить его с помощью флага `--disable-mc-tests`, чтобы было проще проверять решение на других тестах. Однако учтите, что если этот тест упадет с ошибкой, то базовый функционал без обработки отказов засчитан не будет. Исключением является случай, когда данный тест не проходит из-за превышения лимита времени выполнения (5 минут). Это значит, что ваша реализация создает слишком много сообщений или таймеров, скорее всего избыточных. Такая ошибка не помешает пройти группу с базовым функционалом, но с ней нельзя будет получить полный балл за задание. Решения с такой ошибкой также редко проходят тесты на масштабируемость. В этом случае рекомендуется доработать решение, чтобы избавиться от избыточности. По опыту хорошие решения укладываются на этом тесте в несколько десятков секунд.

Если вы найдете ошибки или требования из условий, которые не покрывают наши тесты, то вы можете получить за это бонусные баллы. Подробности [здесь](https://forms.gle/GngHr4jWweEKrf4M6).

## Оценивание

Это задание имеет повышенный вес 1.5, то есть максимум можно набрать **15 баллов**.

Компоненты задания и их вклад в оценку:
- Базовый функционал без обработки отказов (тесты SIMPLE, GET MEMBERS SEMANTICS, RANDOM SEED, PROCESS JOIN/LEAVE и MODEL CHECKING) - 4 балла.
  - Допускается что тест MODEL CHECKING не проходит по лимиту времени
- Обработка отказов процессов (три теста с "PROCESS CRASH" в названии) - 3 балла.
- Обработка отказов сети (остальные тесты кроме "SCALABILITY...") - 3 балла.
- Масштабируемость при росте числа узлов (два теста "SCALABILITY...") - 4 балла.
- Тест MODEL CHECKING не падает по лимиту времени - 1 балл.
- Отчёт с описанием вашего решения в файле `solution/readme.md`. **При отсутствии отчёта проверка производиться не будет.**
